ğŸš€ Summary of Progress & Next Steps for Full-Scale Expansion

â¸»

ğŸ” What We Didd

We successfully built and tested an LLM-based benchmark pipeline for evaluating distractor effects on multiple-choice questions (MCQs) using the RACE dataset. The key steps:

âœ… 1. Multi-Prompt Distractor Generation (multi_prompt.py)
	â€¢	Extracted reading comprehension MCQs from RACE.
	â€¢	Generated weak & moderate distractors using GPT-4o-mini.
	â€¢	Saved distractor sets separately for controlled experiments.

âœ… 2. METEOR Score Computation (score_tracker.py)
	â€¢	Compared generated distractors with expert ones using METEOR.
	â€¢	Stored computed scores in meteor_scores.json.

âœ… 3. LLM Performance Evaluation (test.py)
	â€¢	Queried an LLM to answer questions with different distractor sets.
	â€¢	Checked if the LLM chose the correct answer.
	â€¢	Recorded accuracy in llm_evaluation.json.

âœ… 4. Data Analysis & Trends (data_analysis.py)
	â€¢	Analyzed accuracy fluctuations across weak & moderate distractor sets.
	â€¢	Checked METEOR correlation with model performance.
	â€¢	Visualized trends using bar & line charts.

ğŸ› ï¸ Debugging Fixes
	â€¢	Ensured correct distractor-to-question alignment.
	â€¢	Normalized METEOR scoring for fair comparison.
	â€¢	Fixed issues with identical scores across sets.

â¸»

ğŸ“ˆ Whatâ€™s Next? (Scaling Up to Full Benchmark)

We built a working prototypeâ€”now we expand it into a full-scale benchmark:

ğŸ”¹ Step 1: Increase Dataset Size
	â€¢	Expand from 10 to thousands of MCQs.
	â€¢	Include both high & middle school RACE datasets:

dataset = load_dataset("race", "all")  # Instead of just 'high'


	â€¢	Store datasets in batches to avoid memory overload.

ğŸ”¹ Step 2: Enhance Distractor Generation
	â€¢	Improve multi-prompt diversity:
	â€¢	Weak distractors â†’ More extreme logical fallacies.
	â€¢	Moderate distractors â†’ Ensure subtle but incorrect reasoning.
	â€¢	Increase the number of distractors per question (e.g., 5 instead of 3).
	â€¢	Experiment with different LLMs (e.g., GPT-4, Mistral, Claude).

ğŸ”¹ Step 3: Full LLM Performance Testing
	â€¢	Test with multiple LLMs (not just GPT-4o-mini).
	â€¢	Use APIs from DeepInfra/OpenAI to select models.
	â€¢	Measure response confidence in addition to accuracy.

ğŸ”¹ Step 4: Advanced Evaluation Metrics
	â€¢	Beyond METEOR, add more NLP metrics:
	â€¢	BLEU (n-gram precision).
	â€¢	ROUGE-L (longest common subsequence overlap).
	â€¢	BERTScore (semantic similarity).
	â€¢	Cross-check whether METEOR strongly correlates with accuracy.

ğŸ”¹ Step 5: Large-Scale Data Analysis
	â€¢	Compare LLM performance trends across difficulty levels.
	â€¢	Analyze which distractors confuse LLMs the most.
	â€¢	Test human vs. LLM accuracy on the same distractor sets.

â¸»

ğŸ“Œ Final Goal: A Robust LLM Benchmark

By following these steps, we create a scalable, reusable benchmark:
	â€¢	Thousands of MCQs with generated distractor sets.
	â€¢	Multiple LLMs evaluated under different difficulty conditions.
	â€¢	Comprehensive analysis to detect model weaknesses.

â¸»

ğŸš€ Next Immediate Steps
	1.	Expand multi_prompt.py to process more data.
	2.	Improve distractor quality with better multi-stage prompts.
	3.	Re-run score_tracker.py & data_analysis.py on larger data.
	4.	Start integrating multiple LLMs via API switching.
	5.  MMLU-law SAT
	6.  SpaCy,  

